{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this project, we will implement basic operations of tensorflow which is an Open Source Software Library for Machine Intelligence. \n",
    "\n",
    "Then we introduce neuron network. Then we will dive into two different neural network as convolutional neural network and Recurrent neural network.\n",
    "\n",
    "For the next part, we will use Convolutional neural network for image classification. We will use Recurrent neural network to implement a simple regression model first.\n",
    "\n",
    "Then we try to combine convolutinal neural network and recurrent neural network to build a model for image recognition and image description by words generated by rnn.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "# Tutorial content\n",
    "\n",
    "In this tutorial, we will show how to intall tensorflow and use it.\n",
    "\n",
    "We will introduce modeling of lneural network and hands-on coding in solving problems. Then we wil implement convolutional neural network model to do image classfication.\n",
    "\n",
    "For MNIST dataset image classification problem, convolutional neural network gets an accuracy around 97% which is far more better.\n",
    "\n",
    "\n",
    "We will cover the following topics in this tutorial:\n",
    "* Data Scraping\n",
    "* Installing tensorflow\n",
    "* Tensorflow basics\n",
    "* Neural network for classification\n",
    "* Convolutional neural network\n",
    "* Recurrent neural network\n",
    "* Model for image recognition and description\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we scraple out data for doing the image recognition and word description.\n",
    "\n",
    "We would like to scraple the image data on web and collects their label. Then try to use convolutional neural network for image recoginition and recurrent neural network for the image description by natural languages.\n",
    "\n",
    "\n",
    "This step will be done in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "# Tensorflow install\n",
    "\n",
    "Before getting started, you'll need to install tensorflow.\n",
    "\n",
    "go to this website, and find the Pip Installation section:\n",
    "https://www.tensorflow.org/versions/master/get_started/os_setup.html#download-and-setup\n",
    "\n",
    "For max os, type following command in terminal:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ sudo easy_install pip\n",
    "\n",
    "$ sudo easy_install --upgrade six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select corrent binary to install, I choose to type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ export\n",
    "TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow\n",
    "-0.10.0-py2-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the right python version to install. I choose python 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ sudo pip install --upgrade $TF_BINARY_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then installation is finished. We can run the following code to determine whether tensorflow is installed successfully:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, TensorFlow intalled successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Congratulations, TensorFlow intalled successfully!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, we will be familiar with session, variables, placeholder in tensorflow. If you want to know more about tensorflow, https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html is good reference for python api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session in tensorflow\n",
    "\n",
    "In tensorflow, we always need to create a session, and then use session's run method to run operations.\n",
    "\n",
    "The belowing is matrix multipy example in tensorflow. We first generate two matrix in tensorflow, then using the matmul which is matrix multiply function to get the result. \n",
    "\n",
    "But unlike python, if we stop here, there wil be no multiply operation implemented. Instead, we have to create a session and run the multiply operation in the session.\n",
    "\n",
    "The allowing code shows two ways of using session which have the same effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]]\n",
      "[[14]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3, 1,2]])\n",
    "matrix2 = tf.constant([[2],\n",
    "                       [4],\n",
    "                       [2]])\n",
    "\n",
    "# matrix multiply, which is similar to numpy np.dot(m1, m2)\n",
    "product = tf.matmul(matrix1, matrix2)  \n",
    "\n",
    "# method 1 to use session, create sesson object, and using run method.\n",
    "sess = tf.Session()\n",
    "# using run method to implement operations\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# close this session\n",
    "sess.close()\n",
    "\n",
    "# method 2 to use session. \n",
    "# It will automatically use sess.close() when the code segment is finished.\n",
    "with tf.Session() as sess:\n",
    "    result2 = sess.run(product)\n",
    "    print(result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables in tensorflow\n",
    "\n",
    "unlike python, Varaibles need to be defined with tf.variable(). Before using the variables, we have to use tf.initialize_all_variables() to initialize all variables, else it will not work.\n",
    "\n",
    "In the belowing coding example, we use session mentioned before to run operation. We define variable state and implement add and update operations to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# define a tf variable, value is 0, variable name is counter.\n",
    "state = tf.Variable(0, name='counter')\n",
    "\n",
    "# define a constant 1\n",
    "ten = tf.constant(10)\n",
    "\n",
    "#  tf method to add state and one\n",
    "new_value = tf.add(state, ten)\n",
    "\n",
    "# assign new_value to state\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# must have, else variable will not work\n",
    "init = tf.initialize_all_variables()  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(5):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder in tensorflow\n",
    "\n",
    "In the previous examples, sessions run without assigning values. If we want to assgning values in session and run it, we need to use placeholder.\n",
    "\n",
    "So what is the difference between placeholder and variables? For tensorflow variables, we have to provide initial value when we declare it. But for placeholder, we don't have to provide an initial value and we can specify it at run time with the feed_dict argument inside Session.run. \n",
    "\n",
    "The below examples shows how to use placeholder with a feed_dict dictionary to input values as 7.0 and 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create placeholder\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "\n",
    "# add operation\n",
    "ouput = tf.add(input1, input2)\n",
    "\n",
    "# run session to implement add method\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Activation function\n",
    "\n",
    "There are many kinds of activation functions in neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"nn3.png\">\n",
    "source: https://www.google.com/search?q=relu+and+sigmoid+activation+function&source=lnms&tbm=isch&sa=X&ved=0ahUKEwj0za6l043QAhUN-2MKHWr5BvYQ_AUICCgB&biw=1280&bih=726#imgrc=2bhyZckfNzhU-M%3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Add a layer in neuron network\n",
    "\n",
    "In this part, we practice on how to add a layer in neuron network using tensorflow. Using tf.matmul() to get W * X + b. If activation_function exists, use it to deal with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    \n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a simple computer vision dataset. It consists of images of handwritten digits. We would like to use neural network to find the true digits for the image.\n",
    "\n",
    "If you want to show how to display images through np, http://g.sweyla.com/blog/2012/mnist-numpy/ is a good article. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"mnist1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for neural network to implement image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy is 88.15% after 1000 iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neuron network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using 2- layer neural network for Mnist image dataset classification, we get an accuray as 88%. But there is a more effecitve way. That is to use convolutional neuron network. For a two-layer convolutional neuron network, it can get an accuracy as 96%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neuron network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In image classfication, the position of digit in the image does not influence the value of digit. So we would like to the neural network to share their parameters across space to decrease influence of position. This is one of reasons  using convolutional neuron network.\n",
    "\n",
    "Next, let us find how the convolution is done. You can also check https://classroom.udacity.com/courses/ud730/ which gives an excellent explanation of deep leanring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"cnn1.png\">\n",
    "Patching Process\n",
    "\n",
    "source = https://classroom.udacity.com/courses/ud730/lessons/6377263405/concepts/64063017560923#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above, The rectange with depth, height and width are a image we want to classify.\n",
    "\n",
    "We want to use the patch in graph to tranform the blue x part with depth 3 to red y part with depth k. Weights are set for the patch part, so the number of weights are much less. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the patch size is same with the whole rectangle, this model becomes neural network. \n",
    "\n",
    "The above image shows one convolution process. If we want to deeper the depth of convolutional neural network,\n",
    "it will be as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img source=\"Desktop/cnn1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"cnn2.png\">\n",
    "source = https://classroom.udacity.com/courses/ud730/lessons/6377263405/concepts/64063017560923#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between each layer, they are tranformed by convolution process mentioned before. Layer size changed from 256 * 256 * 3 to 128 * 128 * 16 to... 32 * 32 * 256...\n",
    "\n",
    "For final output, implement a classfier on it to do classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neuron network terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature map: An image has three feature maps as R, G and B colors. After convolution process, it becomes Y with k feature maps like the picutre above shows.\n",
    "\n",
    "Stride: the number of pixels each time the filter shift in the patch process.\n",
    "\n",
    "Valid padding: after patch process, output map size is less than the input map\n",
    "\n",
    "Same padding: after patch proces, output map size is same as the input map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example to help understand these terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"cnnterm2.png\">\n",
    "source: https://arxiv.org/pdf/1603.07285v1.pdf\n",
    "\n",
    "This is a 2D convolution. The input size is 5 * 5. The patch size is 3 * 3. The stride is s1 = s2 = 2. The zero paddign is p1 = p2 = 1. Because same size for output and input, so it is same padding (index 1,2 means axis direction).\n",
    "\n",
    "We can do the convolution without the zero padding as 0 around the outside, then it is valid padding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pooling:\n",
    "\n",
    "Another important technique is to use pooling to improve the convolutional neural network. If we use stride 3 to do patch, we can loss many information during the pathcing process since some information are ignored.\n",
    "\n",
    "So first we can do the 1 stride conv,then using a pooling has a pooling size and pooling stride to a similar job with the patch done before.\n",
    "\n",
    "In the pooling, max pooling returns the max of all xi in the pooling size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example about max_pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"cnnpooling.png\">\n",
    "This is a 2D convolution. The input size is 5 * 5. The stride is s1 = s2 = 1. Pooling is using max pooling, its size is 3 * 3. (index 1,2 means axis direction).\n",
    "\n",
    "source: https://arxiv.org/pdf/1603.07285v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the convolutional neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all the theory background for build cnn model.\n",
    " \n",
    "For one layer, we first define the patch size and their input size and output size. Then initialize the corresponding weights and bias for this patch. Then using strides as 1 * 1 to implement the convolution process. Using a relu function to deal with the ouput and using pooling to scan the ouput to generate a new feature maps.\n",
    " \n",
    "Forward this feature maps as input to next layer.\n",
    "\n",
    "Until the last layer, using softmax to generat probability and croos entropy function to generate the loss function. Then Using AdamOptimizer to optimize it.\n",
    " \n",
    "Code is as following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.0699\n",
      "0.7809\n",
      "0.8715\n",
      "0.9053\n",
      "0.9226\n",
      "0.9296\n",
      "0.9394\n",
      "0.944\n",
      "0.9493\n",
      "0.9508\n",
      "0.9554\n",
      "0.9548\n",
      "0.9609\n",
      "0.9602\n",
      "0.9632\n",
      "0.9652\n",
      "0.9659\n",
      "0.9672\n",
      "0.9693\n",
      "0.969\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# number 1 to 10 digit image classification\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# compute the accuracy\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#  using stride as 1 * 1\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# using pooling as strides 2 * 2, size 2 * 2\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# define placeholder for inputs\n",
    "xs = tf.placeholder(tf.float32, [None, 784]) # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(xs, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "## conv1 layer ##\n",
    "# patch 5x5, size is 1, out size 32\n",
    "W_conv1 = weight_variable([5,5, 1,32]) \n",
    "b_conv1 = bias_variable([32])\n",
    "# output size is 28x28x32\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) \n",
    "# output size is 14x14x32\n",
    "h_pool1 = max_pool_2x2(h_conv1)                                         \n",
    "\n",
    "## conv2 layer ##\n",
    "# patch 5x5, in size 32, out size 64\n",
    "W_conv2 = weight_variable([5,5, 32, 64]) \n",
    "b_conv2 = bias_variable([64])\n",
    "# output size is 14x14x64\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) \n",
    "# output size is 7x7x64\n",
    "h_pool2 = max_pool_2x2(h_conv2)                                        \n",
    "\n",
    "## func1 layer ##\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "# [n_samples, 7, 7, 64] to [n_samples, 7*7*64]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#  this is using dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "## func2 layer ##\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "    if i % 50 == 0:\n",
    "        print(compute_accuracy(\n",
    "            mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final accuray is around 97% which is much better than the neural network as 88%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Recurrent neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data which has a sequence, we would like to use the Recurrent neural network to do predict.\n",
    "\n",
    "<img src = \"rnn1.png\">\n",
    "source: https://www.google.com/search?q=rnn+image&biw=1280&bih=703&source=lnms&tbm=isch&sa=X&ved=0ahUKEwiXlK_svbvQAhUF4CYKHZhxAiEQ_AUICCgD#imgrc=xxI0myG53kUNdM%3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement the recurrent neural network for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BATCH_START = 0\n",
    "TIME_STEPS = 20\n",
    "BATCH_SIZE = 50\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "CELL_SIZE = 10\n",
    "LR = 0.006\n",
    "\n",
    "\n",
    "def get_batch():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    # xs shape (50batch, 20steps)\n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    # plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')\n",
    "    # plt.show()\n",
    "    # returned seq, res and xs: shape (batch, step, input)\n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]\n",
    "\n",
    "\n",
    "class LSTMRNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cell()\n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "\n",
    "    def add_input_layer(self,):\n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D')  # (batch*n_step, in_size)\n",
    "        # Ws (in_size, cell_size)\n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        # bs (cell_size, )\n",
    "        bs_in = self._bias_variable([self.cell_size,])\n",
    "        # l_in_y = (batch * n_steps, cell_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "        # reshape l_in_y ==> (batch, n_steps, cell_size)\n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "\n",
    "    def add_cell(self):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)\n",
    "\n",
    "    def add_output_layer(self):\n",
    "        # shape = (batch * steps, cell_size)\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([self.output_size, ])\n",
    "        # shape = (batch * steps, output_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out\n",
    "\n",
    "    def compute_cost(self):\n",
    "        losses = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps=True,\n",
    "            softmax_loss_function=self.ms_error,\n",
    "            name='losses'\n",
    "        )\n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(\n",
    "                tf.reduce_sum(losses, name='losses_sum'),\n",
    "                self.batch_size,\n",
    "                name='average_cost')\n",
    "            tf.scalar_summary('cost', self.cost)\n",
    "\n",
    "    def ms_error(self, y_pre, y_target):\n",
    "        return tf.square(tf.sub(y_pre, y_target))\n",
    "\n",
    "    def _weight_variable(self, shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.,)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "\n",
    "    def _bias_variable(self, shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "    sess = tf.Session()\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"logs\", sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # relocate to the local dir and run this line to view it on Chrome (http://0.0.0.0:6006/):\n",
    "    # $ tensorboard --logdir='logs'\n",
    "\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "    for i in range(200):\n",
    "        seq, res, xs = get_batch()\n",
    "        if i == 0:\n",
    "            feed_dict = {\n",
    "                    model.xs: seq,\n",
    "                    model.ys: res,\n",
    "                    # create initial state\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                model.xs: seq,\n",
    "                model.ys: res,\n",
    "                model.cell_init_state: state    # use last state as the initial state for this run\n",
    "            }\n",
    "\n",
    "        _, cost, state, pred = sess.run(\n",
    "            [model.train_op, model.cost, model.cell_final_state, model.pred],\n",
    "            feed_dict=feed_dict)\n",
    "\n",
    "        # plotting\n",
    "        # plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')\n",
    "        # plt.ylim((-1.2, 1.2))\n",
    "        # plt.draw()\n",
    "        # plt.pause(0.3)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print('cost: ', round(cost, 4))\n",
    "            result = sess.run(merged, feed_dict)\n",
    "            writer.add_summary(result, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model of recurrent neural network and convolutional neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO be continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More references are available:\n",
    "\n",
    "1 tensorflow: https://www.tensorflow.org/\n",
    "\n",
    "2 Machine Learning: https://www.coursera.org/learn/machine-learning\n",
    "\n",
    "3 deep learning: https://classroom.udacity.com/courses/ud730\n",
    "\n",
    "4 cs231n: http://cs231n.stanford.edu/\n",
    "\n",
    "5 a guide to convolution arithmetic: https://arxiv.org/pdf/1603.07285v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
